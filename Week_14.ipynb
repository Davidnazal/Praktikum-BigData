{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8998d0d1-c1b5-43b0-a692-f77af733456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 10:18:20,732 WARN util.Utils: Your hostname, david resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "2025-11-26 10:18:20,739 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-11-26 10:18:23,755 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-11-26 10:18:24,034 WARN spark.SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "2025-11-26 10:18:56,243 WARN util.Instrumentation: [27dcc3cb] regParam is zero, which might cause numerical instability and overfitting.\n",
      "2025-11-26 10:19:02,575 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "2025-11-26 10:19:02,591 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "2025-11-26 10:19:03,151 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "# Load sample data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e4e70a-140a-4fca-9431-818a4292428b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057936866103,4.0873522690497905]\n",
      "Intercept: 11.568912734332656\n",
      "+---+---------+-----+--------------------+--------------------+----------+\n",
      "| ID| Features|Label|       rawPrediction|         probability|prediction|\n",
      "+---+---------+-----+--------------------+--------------------+----------+\n",
      "|  1|[2.0,3.0]|    0|[0.69314633225017...|[0.66666647815335...|       0.0|\n",
      "|  2|[1.0,5.0]|    1|[-19.743616142715...|[2.66352302390614...|       1.0|\n",
      "|  3|[2.5,4.5]|    1|[0.69314689710854...|[0.66666660367746...|       0.0|\n",
      "|  4|[3.0,6.0]|    0|[0.69314746196691...|[0.66666672920154...|       0.0|\n",
      "+---+---------+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Practice: Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Example dataset dengan Vector\n",
    "data = [\n",
    "    (1, Vectors.dense([2.0, 3.0]), 0),\n",
    "    (2, Vectors.dense([1.0, 5.0]), 1),\n",
    "    (3, Vectors.dense([2.5, 4.5]), 1),\n",
    "    (4, Vectors.dense([3.0, 6.0]), 0)\n",
    "]\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Display coefficients and summary\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')\n",
    "\n",
    "# Optional: Show predictions\n",
    "predictions = model.transform(df)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c8b9f4-4a4e-40db-ba48-8cc67517150f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: <bound method KMeansModel.clusterCenters of KMeansModel: uid=KMeans_9b76f68cee62, k=2, distanceMeasure=euclidean, numFeatures=2>\n",
      "+---+-----------+----------+\n",
      "| ID|   Features|prediction|\n",
      "+---+-----------+----------+\n",
      "|  1|  [1.0,1.0]|         1|\n",
      "|  2|  [5.0,5.0]|         1|\n",
      "|  3|[10.0,10.0]|         0|\n",
      "|  4|[15.0,15.0]|         0|\n",
      "+---+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Practice: KMeans Clustering\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Example dataset dengan Vector\n",
    "data = [\n",
    "    (1, Vectors.dense([1.0, 1.0])),\n",
    "    (2, Vectors.dense([5.0, 5.0])),\n",
    "    (3, Vectors.dense([10.0, 10.0])),\n",
    "    (4, Vectors.dense([15.0, 15.0]))\n",
    "]\n",
    "columns = ['ID', 'Features']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train KMeans clustering model\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df)\n",
    "\n",
    "# Show cluster centers\n",
    "centers = model.clusterCenters\n",
    "print(f'Cluster Centers: {centers}')\n",
    "\n",
    "# Optional: Show predictions\n",
    "predictions = model.transform(df)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aa539e3-d096-4ecf-99b4-209f74eec6a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 22:24:13,406 WARN spark.SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 22\n",
      "+--------------+-----+\n",
      "|      Features|Label|\n",
      "+--------------+-----+\n",
      "|[25.0,35000.0]|  0.0|\n",
      "|[28.0,42000.0]|  0.0|\n",
      "|[22.0,30000.0]|  0.0|\n",
      "|[26.0,38000.0]|  0.0|\n",
      "|[24.0,32000.0]|  0.0|\n",
      "|[45.0,85000.0]|  1.0|\n",
      "|[50.0,90000.0]|  1.0|\n",
      "|[42.0,80000.0]|  1.0|\n",
      "|[48.0,95000.0]|  1.0|\n",
      "|[44.0,87000.0]|  1.0|\n",
      "|[35.0,55000.0]|  0.0|\n",
      "|[35.0,55000.0]|  1.0|\n",
      "|[40.0,60000.0]|  0.0|\n",
      "|[40.0,60000.0]|  1.0|\n",
      "|[30.0,75000.0]|  0.0|\n",
      "|[30.0,75000.0]|  1.0|\n",
      "|[45.0,45000.0]|  0.0|\n",
      "|[45.0,45000.0]|  1.0|\n",
      "|[33.0,48000.0]|  1.0|\n",
      "|[38.0,52000.0]|  0.0|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Initial AUC: 0.9000\n",
      "\n",
      "Predictions (focus on challenging samples):\n",
      "+--------------+-----+----------+\n",
      "|      Features|Label|prediction|\n",
      "+--------------+-----+----------+\n",
      "|[29.0,68000.0]|  1.0|       0.0|\n",
      "|[40.0,60000.0]|  0.0|       1.0|\n",
      "+--------------+-----+----------+\n",
      "\n",
      "\n",
      "Correct predictions:\n",
      "+--------------+-----+----------+\n",
      "|      Features|Label|prediction|\n",
      "+--------------+-----+----------+\n",
      "|[25.0,35000.0]|  0.0|       0.0|\n",
      "|[42.0,80000.0]|  1.0|       1.0|\n",
      "|[45.0,85000.0]|  1.0|       1.0|\n",
      "|[48.0,95000.0]|  1.0|       1.0|\n",
      "|[38.0,52000.0]|  0.0|       0.0|\n",
      "+--------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Model coefficients:\n",
      "Coefficients: [0.09774070343953856,5.834913337989421e-05]\n",
      "Intercept: -7.027490718017541\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('MLlib Homework').getOrCreate()\n",
    "\n",
    "data = [\n",
    "   \n",
    "    (Vectors.dense([25.0, 35000.0]), 0.0),\n",
    "    (Vectors.dense([28.0, 42000.0]), 0.0),\n",
    "    (Vectors.dense([22.0, 30000.0]), 0.0),\n",
    "    (Vectors.dense([26.0, 38000.0]), 0.0),\n",
    "    (Vectors.dense([24.0, 32000.0]), 0.0),\n",
    "    (Vectors.dense([45.0, 85000.0]), 1.0),\n",
    "    (Vectors.dense([50.0, 90000.0]), 1.0),\n",
    "    (Vectors.dense([42.0, 80000.0]), 1.0),\n",
    "    (Vectors.dense([48.0, 95000.0]), 1.0),\n",
    "    (Vectors.dense([44.0, 87000.0]), 1.0),\n",
    "    (Vectors.dense([35.0, 55000.0]), 0.0), \n",
    "    (Vectors.dense([35.0, 55000.0]), 1.0), \n",
    "    (Vectors.dense([40.0, 60000.0]), 0.0),\n",
    "    (Vectors.dense([40.0, 60000.0]), 1.0),  \n",
    "    (Vectors.dense([30.0, 75000.0]), 0.0),  \n",
    "    (Vectors.dense([30.0, 75000.0]), 1.0),  \n",
    "    (Vectors.dense([45.0, 45000.0]), 0.0),\n",
    "    (Vectors.dense([45.0, 45000.0]), 1.0),  \n",
    "    (Vectors.dense([33.0, 48000.0]), 1.0),  \n",
    "    (Vectors.dense([38.0, 52000.0]), 0.0),  \n",
    "    (Vectors.dense([29.0, 68000.0]), 1.0),  \n",
    "    (Vectors.dense([41.0, 58000.0]), 0.0),  \n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, ['Features', 'Label'])\n",
    "print(\"Dataset size:\", df.count())\n",
    "df.show()\n",
    "\n",
    "\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Label')\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Initial AUC: {auc:.4f}\")\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.001, 0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [10, 50]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cv_model = crossval.fit(train_data)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "best_predictions = best_model.transform(test_data)\n",
    "best_auc = evaluator.evaluate(best_predictions)\n",
    "\n",
    "\n",
    "print(\"\\nPredictions (focus on challenging samples):\")\n",
    "best_predictions.select('Features', 'Label', 'prediction').filter('Label != prediction').show()\n",
    "\n",
    "print(\"\\nCorrect predictions:\")\n",
    "best_predictions.select('Features', 'Label', 'prediction').filter('Label = prediction').show(5)\n",
    "\n",
    "\n",
    "print(\"\\nModel coefficients:\")\n",
    "print(f\"Coefficients: {best_model.coefficients}\")\n",
    "print(f\"Intercept: {best_model.intercept}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1ce5a-bb47-4fbc-ab96-98e5355ffee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_env)",
   "language": "python",
   "name": "spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
